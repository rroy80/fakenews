{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b577ce5d-397c-4d1c-abda-0b3c64ef48e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "465347c0-f392-4af4-9061-fe2a3ce4e5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "truenews = pd.read_csv('./data/True.csv') \n",
    "#truenews.head()\n",
    "fakenews = pd.read_csv('./data/Fake.csv') \n",
    "#fakenews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "787552ba-5c83-4f36-bd50-b23c3c67f9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "truenews = truenews.drop([ \"subject\",\"date\"], axis = 1)\n",
    "fakenews = fakenews.drop([ \"subject\",\"date\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "864b0060-112a-4457-8501-d3771471c0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "truenews['result'] = 0\n",
    "fakenews['result'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36708181-e9da-4bc9-9aeb-860b393c11ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [truenews, fakenews]\n",
    "result = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35616a54-b901-437c-a988-9e68e99de0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "string.punctuation\n",
    "def remove_punct(txt):\n",
    "    text_remove_punct = [c for c in txt if c not in string.punctuation]\n",
    "    return text_remove_punct\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatiz(txt):\n",
    "    txt = lemmatizer.lemmatize(txt)\n",
    "    #txt = txt.split()\n",
    "    return txt\n",
    "\n",
    "def regular_expr(txt):\n",
    "    txt = re.sub('[^a-zA-Z]',' ',str(txt))\n",
    "    #txt = txt.split()\n",
    "    return txt\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(txt):\n",
    "    txt = [i for i in txt if i not in stop_words ]\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15dac146-62ea-4e87-8598-d3a623407921",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "result['text'] = result['text'].map(lambda x:  x.lower())\n",
    "result['text'] = result['text'].map(lambda x:  regular_expr(x))\n",
    "result['text'] = result['text'].map(lambda x:  lemmatiz(x))\n",
    "#result['text'] = result['text'].map(lambda x:  word_tokenize(x))\n",
    "#result['text'] = result['text'].map(lambda x:  remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf7438d8-260a-436e-a8cd-a2f60a552424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "msk = np.random.rand(len(result)) < 0.8\n",
    "train = result[msk].copy()\n",
    "test=result[~msk].copy()\n",
    "#print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4548ac8-4e20-4d35-8afc-82999fa94202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features = 150000,lowercase = False, ngram_range = (1,2),stop_words = 'english', max_df = 0.7)\n",
    "#tfidf = TfidfVectorizer(stop_words = 'english', max_df = 0.7)\n",
    "x_train = tfidf.fit_transform(train['text'])\n",
    "x_test = tfidf.fit_transform(test['text'])\n",
    "#print(tfidf.fit_transform(result['text'][0]))\n",
    "#def regular_vectorize(txt):\n",
    "#   return tfidf.fit_transform(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcff4c7b-f8e5-44e4-8ee2-88840187438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB \n",
    "\n",
    "clf = MultinomialNB()\n",
    "y_train = train['result'].to_numpy()\n",
    "clf.fit(x_train,y_train)\n",
    "predict = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b4a8bf7-965c-416d-90cf-283fa21c1442",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test['result'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b31bf6c-16ba-4432-8262-c351b758ddf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.5678817289245006\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] == predict[i]:\n",
    "        count+=1\n",
    "k = count/len(y_test)\n",
    "print(f'Accuracy : {k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cf2b8f3-f860-4e4f-9ac7-2ff21f46f564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9617892721369703\n"
     ]
    }
   ],
   "source": [
    "predict = clf.predict(x_train)\n",
    "count = 0\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] == predict[i]:\n",
    "        count+=1\n",
    "k = count/len(y_train)\n",
    "print(f'Accuracy : {k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def74407-6835-4768-9ebd-df66e48ccbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d877f1b4-9876-4d5d-b71f-cf4d08f2f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c43a3ad-5180-4517-8ef3-c14054ee3b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm.fit(x_train.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2d54b2-ed38-428b-aec2-3a6092e3268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = gmm.predict(x_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7054ed5-c2dd-4408-9913-f0ea77342dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] == predict[i]:\n",
    "        count+=1\n",
    "k = count/len(y_test)\n",
    "print(f'Accuracy : {k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08af152b-bd42-4e94-90cb-5408033b1b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38d0bf8-34f6-4e61-84c2-592fe37d0713",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "cls = svm.SVC(kernel=\"linear\")\n",
    "y_train = train['result'].to_numpy()\n",
    "cls.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997da30c-6e7e-4b02-84b8-63f83b1f0e76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1414fde-f944-4da7-84b3-dfbdea9db54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(8, 8, 8), max_iter=100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(8, 8, 8), max_iter=100)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(8, 8, 8), max_iter=100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "y_train = train['result'].to_numpy()\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(8,8,8), activation='relu', solver='adam', max_iter=100)\n",
    "mlp.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6a39c01-89e1-4e98-9d8a-8022d0110c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = mlp.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebbfe3bb-d2fc-4597-ab72-12a6d561965b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.5187201422064215\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "y_test = test['result'].to_numpy()\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] == predict[i]:\n",
    "        count+=1\n",
    "k = count/len(y_test)\n",
    "print(f'Accuracy : {k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce23d9f9-677c-4023-8e82-41d01bb9b496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a587d4f-c43f-4a8f-b67f-48798b309c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e264e988-e5d6-49c2-8476-331e87053fea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70511d0-f731-48b4-962b-6b63514795cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea39f86-9fac-44c5-9a65-f346668b75c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d774e89-47e8-4a27-8f0a-443bae972b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe711a30-c6a7-4e42-a99e-b49ecbe329d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2402d8b4-47d5-4185-8d58-718e4f5c2a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "72ba7f99-583e-4bf2-859b-6c560e508067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(txt):\n",
    "    text_remove_punct = [c for c in txt if c not in string.punctuation]\n",
    "    return text_remove_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a97ab6be-7968-43d0-ac86-31d6da8e777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def regular_expr(txt):\n",
    "    txt = re.sub('[^a-zA-Z]',' ',str(txt))\n",
    "    #txt = txt.split()\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f2f0cb5e-a060-4ce6-baf6-dc6b66bb1fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train['text'] = train['text'].map(lambda x:  remove_punct(x))\n",
    "#test['text'] = test['text'].map(lambda x:  remove_punct(x))\n",
    "\n",
    "train['text'] = train['text'].map(lambda x:  regular_expr(x))\n",
    "test['text'] = test['text'].map(lambda x:  regular_expr(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "be978e5a-4bd7-493f-adf8-ff134a90c021",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result['text'] = result['text'].map(lambda x:  regular_expr(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1a8c84f8-4ecd-4abb-bf5e-d45fb68048ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12       the following statements were posted to the ve...\n",
      "14         in dec     story  in second paragraph  corre...\n",
      "24       lima  reuters    peru s president pedro pablo ...\n",
      "32       washington  reuters    steve bannon  a former ...\n",
      "37        reuters    the u s  congress on thursday appr...\n",
      "                               ...                        \n",
      "23462    tune in to the alternate current radio network...\n",
      "23463    patrick henningsen   st century wire update   ...\n",
      "23472     by dady chery and gilbert mercierall writers ...\n",
      "23476      st century wire says as   wire reported earl...\n",
      "23477      st century wire says it s a familiar theme  ...\n",
      "Name: text, Length: 9013, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e33770a8-9885-4e43-9de2-2848a24b5fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatiz(txt):\n",
    "    txt = lemmatizer.lemmatize(txt)\n",
    "    #txt = txt.split()\n",
    "    return txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "58ffd884-a455-4144-b65b-905ea682bdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].map(lambda x:  lemmatiz(x))\n",
    "test['text'] = test['text'].map(lambda x:  lemmatiz(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "96ed29f5-2512-4f89-a8f1-37e6a87475e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        washington  reuters    the head of a conservat...\n",
      "1        washington  reuters    transgender people will...\n",
      "2        washington  reuters    the special counsel inv...\n",
      "3        washington  reuters    trump campaign adviser ...\n",
      "4        seattle washington  reuters    president donal...\n",
      "                               ...                        \n",
      "23474    paul craig robertsin the last years of the   t...\n",
      "23475    robert fantina counterpunchalthough the united...\n",
      "23478    patrick henningsen    st century wireremember ...\n",
      "23479      st century wire says al jazeera america will...\n",
      "23480      st century wire says as   wire predicted in ...\n",
      "Name: text, Length: 35885, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "51a32afe-b748-4e26-af50-eadf2f4ae6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_token(txt):\n",
    "    #txt = re.sub('[^a-zA-Z]',' ',str(txt))\n",
    "    txt = txt.split()\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6aa26a00-232f-489e-8570-87b9a5f2c05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train['text'] = train['text'].map(lambda x:  regular_token(x))\n",
    "#test['text'] = test['text'].map(lambda x:  regular_token(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "61e9710b-7b5c-4755-a036-18101adf9b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        washington  reuters    the head of a conservat...\n",
      "1        washington  reuters    transgender people will...\n",
      "2        washington  reuters    the special counsel inv...\n",
      "3        washington  reuters    trump campaign adviser ...\n",
      "4        seattle washington  reuters    president donal...\n",
      "                               ...                        \n",
      "23474    paul craig robertsin the last years of the   t...\n",
      "23475    robert fantina counterpunchalthough the united...\n",
      "23478    patrick henningsen    st century wireremember ...\n",
      "23479      st century wire says al jazeera america will...\n",
      "23480      st century wire says as   wire predicted in ...\n",
      "Name: text, Length: 35885, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c0397ac2-6f37-4df6-a255-d0ff5fe55315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features = 125772,lowercase = False, ngram_range = (1,2))\n",
    "x_train = tfidf.fit_transform(train['text'])\n",
    "x_test = tfidf.fit_transform(test['text'])\n",
    "#print(tfidf.fit_transform(result['text'][0]))\n",
    "#def regular_vectorize(txt):\n",
    "#   return tfidf.fit_transform(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5dfb98a5-d1b7-439b-8b0a-c8a75688635c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result['count'] = result['text'].map(lambda x:  regular_vectorize([x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2a5d0eb3-8b69-41d8-99f8-7d6233a86467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9013, 125772)\n",
      "(35885, 125772)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9b49d3b4-3483-465d-9a55-9017284b1f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.count of                                                    title  \\\n",
      "0      as u.s. budget fight looms, republicans flip t...   \n",
      "1      u.s. military to accept transgender recruits o...   \n",
      "2      senior u.s. republican senator: 'let mr. muell...   \n",
      "3      fbi russia probe helped by australian diplomat...   \n",
      "4      trump wants postal service to charge 'much mor...   \n",
      "...                                                  ...   \n",
      "23476  mcpain: john mccain furious that iran treated ...   \n",
      "23477  justice? yahoo settles e-mail privacy class-ac...   \n",
      "23478  sunnistan: us and allied ‘safe zone’ plan to t...   \n",
      "23479  how to blow $700 million: al jazeera america f...   \n",
      "23480  10 u.s. navy sailors held by iranian military ...   \n",
      "\n",
      "                                                    text  result  \n",
      "0      washington  reuters    the head of a conservat...       0  \n",
      "1      washington  reuters    transgender people will...       0  \n",
      "2      washington  reuters    the special counsel inv...       0  \n",
      "3      washington  reuters    trump campaign adviser ...       0  \n",
      "4      seattle washington  reuters    president donal...       0  \n",
      "...                                                  ...     ...  \n",
      "23476    st century wire says as   wire reported earl...       1  \n",
      "23477    st century wire says it s a familiar theme  ...       1  \n",
      "23478  patrick henningsen    st century wireremember ...       1  \n",
      "23479    st century wire says al jazeera america will...       1  \n",
      "23480    st century wire says as   wire predicted in ...       1  \n",
      "\n",
      "[44898 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(result.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e524ae0d-2a1a-477f-8d56-e9c1fcd4365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB \n",
    "\n",
    "clf = MultinomialNB()\n",
    "y_train = train['result'].to_numpy()\n",
    "clf.fit(x_train,y_train)\n",
    "predict = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c58ebda7-fc92-4114-8ae2-4f47d0f5b249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c386bcc1-f51b-4561-a247-f7e69fee93de",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test['result'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5da8ec69-063b-4dcc-ae5b-77adaf7c56f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f46a3ec1-e634-4523-8f28-57575bf1ebba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6174414734272717\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] == predict[i]:\n",
    "        count+=1\n",
    "k = count/len(y_test)\n",
    "print(f'Accuracy : {k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f3414a5c-ed3a-45b7-9759-c32c68876a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = clf.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a00d4577-e3c6-40e7-a76c-8458740bdca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.960484882262784\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] == predict[i]:\n",
    "        count+=1\n",
    "k = count/len(y_train)\n",
    "print(f'Accuracy : {k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc901e50-d142-470a-8719-47405cc26535",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
